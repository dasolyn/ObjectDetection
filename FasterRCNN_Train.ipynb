{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Faster RCNN 모델 훈련 과정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import random\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "from six.moves import range\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras_frcnn import config, data_generators\n",
    "from keras_frcnn import losses as losses\n",
    "import keras_frcnn.roi_helpers as roi_helpers\n",
    "from keras.utils import generic_utils\n",
    "from keras_frcnn.pascal_voc_parser import get_data\n",
    "from keras_frcnn import resnet as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 설정\n",
    "\n",
    "아래는 데이터셋 및 훈련 설정에 필요한 환경 변수입니다. 사용하는 데이터셋 및 모델에 맞춰서 변경하시기 바랍니다.\n",
    "\n",
    " - `dataset_path` : 데이터셋이 존재하는 경로를 지정합니다.\n",
    " - `train_dataset_name` : 데이터셋 내에서 훈련용으로 사용할 ImageSets 묶음의 이름입니다.\n",
    " - `val_dataset_name` : 데이터셋 내에서 검증용으로 사용할 ImageSets 묶음의 이름입니다.\n",
    " - `weight_file_path` : 결과 weight파일이 출력될 경로 및 이름을 지정합니다.\n",
    " - `config_file_path` : 모델 정보를 담은 pickle 파일이 출력될 경로 및 이름을 지정합니다.\n",
    " - `epochs` : 총 몇 epoch를 실행할 건지를 지정할 수 있습니다.\n",
    " - `epoch_len` : 한 epoch당 몇 샘플을 훈련할 것인지를 지정할 수 있습니다. 이 값이 1보다 작으면 한 epoch마다 훈련용 데이터셋 전체를 순회합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_path = './dataset/PascalVOC2012'\n",
    "train_dataset_name = 'train'\n",
    "val_dataset_name = 'val'\n",
    "weight_file_path = './frcnn.hdf5'\n",
    "config_file_path = './config.pickle'\n",
    "epochs = 10\n",
    "epoch_len = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 로딩\n",
    "\n",
    "훈련 및 검증에 필요한 데이터셋을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing annotation files\n"
     ]
    }
   ],
   "source": [
    "C = config.Config()\n",
    "\n",
    "C.use_horizontal_flips = True\n",
    "C.use_vertical_flips = True\n",
    "C.rot_90 = False\n",
    "\n",
    "all_imgs, classes_count, class_mapping = get_data(dataset_path, train_dataset_name, val_dataset_name)\n",
    "\n",
    "if 'bg' not in classes_count:\n",
    "    classes_count['bg'] = 0\n",
    "class_mapping['bg'] = len(class_mapping)\n",
    "\n",
    "C.class_mapping = class_mapping\n",
    "\n",
    "inv_map = {v: k for k, v in class_mapping.items()}\n",
    "\n",
    "random.shuffle(all_imgs)\n",
    "\n",
    "num_imgs = len(all_imgs)\n",
    "\n",
    "train_imgs = [s for s in all_imgs if s['imageset'] == train_dataset_name]\n",
    "val_imgs = [s for s in all_imgs if s['imageset'] == val_dataset_name]\n",
    "\n",
    "data_gen_train = data_generators.get_anchor_gt(train_imgs, classes_count, C, nn.get_img_output_length, K.image_dim_ordering(), mode='train')\n",
    "data_gen_val = data_generators.get_anchor_gt(val_imgs, classes_count, C, nn.get_img_output_length,K.image_dim_ordering(), mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images per class:\n",
      "{'aeroplane': 1002,\n",
      " 'bg': 0,\n",
      " 'bicycle': 837,\n",
      " 'bird': 1271,\n",
      " 'boat': 1059,\n",
      " 'bottle': 1561,\n",
      " 'bus': 685,\n",
      " 'car': 2492,\n",
      " 'cat': 1277,\n",
      " 'chair': 3056,\n",
      " 'cow': 771,\n",
      " 'diningtable': 800,\n",
      " 'dog': 1598,\n",
      " 'horse': 803,\n",
      " 'motorbike': 801,\n",
      " 'person': 17401,\n",
      " 'pottedplant': 1202,\n",
      " 'sheep': 1084,\n",
      " 'sofa': 841,\n",
      " 'train': 704,\n",
      " 'tvmonitor': 893}\n",
      "Num classes (including bg) = 21\n",
      "Num train samples 11302\n",
      "Num val samples 5823\n"
     ]
    }
   ],
   "source": [
    "print('Training images per class:')\n",
    "pprint.pprint(classes_count)\n",
    "print('Num classes (including bg) = {}'.format(len(classes_count)))\n",
    "print('Num train samples {}'.format(len(train_imgs)))\n",
    "print('Num val samples {}'.format(len(val_imgs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 빌드\n",
    "\n",
    "Faster RCNN with ResNet50 모델을 빌드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config has been written to ./config.pickle, and can be loaded when testing to ensure correct results\n"
     ]
    }
   ],
   "source": [
    "C.model_path = weight_file_path\n",
    "C.num_rois = 32\n",
    "C.network = 'resnet50'\n",
    "C.base_net_weights = './keras_frcnn/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "\n",
    "with open(config_file_path, 'wb') as config_f:\n",
    "    pickle.dump(C, config_f)\n",
    "print('Config has been written to {}, and can be loaded when testing to ensure correct results'.format(config_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from ./keras_frcnn/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n"
     ]
    }
   ],
   "source": [
    "img_input = Input(shape=(None, None, 3))\n",
    "roi_input = Input(shape=(None, 4))\n",
    "shared_layers = nn.nn_base(img_input, trainable=True)\n",
    "\n",
    "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\n",
    "rpn = nn.rpn(shared_layers, num_anchors)\n",
    "\n",
    "classifier = nn.classifier(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count), trainable=True)\n",
    "\n",
    "model_rpn = Model(img_input, rpn[:2])\n",
    "model_classifier = Model([img_input, roi_input], classifier)\n",
    "\n",
    "model_all = Model([img_input, roi_input], rpn[:2] + classifier)\n",
    "\n",
    "print('loading weights from {}'.format(C.base_net_weights))\n",
    "model_rpn.load_weights(C.base_net_weights, by_name=True)\n",
    "model_classifier.load_weights(C.base_net_weights, by_name=True)\n",
    "\n",
    "optimizer = Adam(lr=1e-5)\n",
    "optimizer_classifier = Adam(lr=1e-5)\n",
    "model_rpn.compile(optimizer=optimizer, loss=[losses.rpn_loss_cls(num_anchors), losses.rpn_loss_regr(num_anchors)])\n",
    "model_classifier.compile(optimizer=optimizer_classifier, loss=[losses.class_loss_cls, losses.class_loss_regr(len(classes_count)-1)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})\n",
    "model_all.compile(optimizer='sgd', loss='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 시작\n",
    "\n",
    "데이터셋에 맞게 모델의 훈련을 시작합니다. 훈련 과정에서 val_loss가 최저인 weight가 자동으로 저장됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch 1/10\n",
      " 982/1000 [============================>.] - ETA: 15s - rpn_cls: 3.2780 - rpn_regr: 0.1723 - detector_cls: 1.3312 - detector_regr: 0.4273Average number of overlapping bounding boxes from RPN = 13.463 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 871s 871ms/step - rpn_cls: 3.2686 - rpn_regr: 0.1723 - detector_cls: 1.3275 - detector_regr: 0.4271\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 13.5343811395\n",
      "Classifier accuracy for bounding boxes from RPN: 0.68409375\n",
      "Loss RPN classifier: 2.75721625919\n",
      "Loss RPN regression: 0.169774478639\n",
      "Loss Detector classifier: 1.12017383945\n",
      "Loss Detector regression: 0.415453916684\n",
      "Elapsed time: 870.557617903\n",
      "Total loss decreased from inf to 4.46261849396, saving weights\n",
      "Epoch 2/10\n",
      " 976/1000 [============================>.] - ETA: 17s - rpn_cls: 2.0553 - rpn_regr: 0.1667 - detector_cls: 0.9033 - detector_regr: 0.4009Average number of overlapping bounding boxes from RPN = 14.738 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 721s 721ms/step - rpn_cls: 2.0558 - rpn_regr: 0.1668 - detector_cls: 0.9026 - detector_regr: 0.4007\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 14.7216699801\n",
      "Classifier accuracy for bounding boxes from RPN: 0.6989375\n",
      "Loss RPN classifier: 2.07846359299\n",
      "Loss RPN regression: 0.169896317174\n",
      "Loss Detector classifier: 0.872685098105\n",
      "Loss Detector regression: 0.393101106316\n",
      "Elapsed time: 724.95783186\n",
      "Total loss decreased from 4.46261849396 to 3.51414611458, saving weights\n",
      "Epoch 3/10\n",
      " 548/1000 [===============>..............] - ETA: 5:25 - rpn_cls: 1.8128 - rpn_regr: 0.1671 - detector_cls: 0.8521 - detector_regr: 0.3885"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-473c702982e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mP_rpn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_rpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroi_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn_to_roi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP_rpn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP_rpn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dim_ordering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_regr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_boxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIouS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroi_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_iou\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/jupyter/objectdetection/keras_frcnn/roi_helpers.pyc\u001b[0m in \u001b[0;36mrpn_to_roi\u001b[0;34m(rpn_layer, regr_layer, C, dim_ordering, use_regr, max_boxes, overlap_thresh)\u001b[0m\n\u001b[1;32m    268\u001b[0m                         \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_layer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m                         \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_layer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m                         \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_layer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                         \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_layer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iter_num = 0\n",
    "\n",
    "losses = np.zeros((epoch_len, 5))\n",
    "rpn_accuracy_rpn_monitor = []\n",
    "rpn_accuracy_for_epoch = []\n",
    "start_time = time.time()\n",
    "\n",
    "best_loss = np.Inf\n",
    "\n",
    "class_mapping_inv = {v: k for k, v in class_mapping.items()}\n",
    "print('Starting training')\n",
    "\n",
    "for epoch_num in range(epochs):\n",
    "\n",
    "    progbar = generic_utils.Progbar(epoch_len)\n",
    "    print('Epoch {}/{}'.format(epoch_num + 1, epochs))\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            if len(rpn_accuracy_rpn_monitor) == epoch_len and C.verbose:\n",
    "                mean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)\n",
    "                rpn_accuracy_rpn_monitor = []\n",
    "                print('Average number of overlapping bounding boxes from RPN = {} for {} previous iterations'.format(mean_overlapping_bboxes, epoch_len))\n",
    "                if mean_overlapping_bboxes == 0:\n",
    "                    print('RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.')\n",
    "\n",
    "            # RPN 훈련\n",
    "            X, Y, img_data = next(data_gen_train)\n",
    "\n",
    "            loss_rpn = model_rpn.train_on_batch(X, Y)\n",
    "\n",
    "            P_rpn = model_rpn.predict_on_batch(X)\n",
    "\n",
    "            R = roi_helpers.rpn_to_roi(P_rpn[0], P_rpn[1], C, K.image_dim_ordering(), use_regr=True, overlap_thresh=0.7, max_boxes=300)\n",
    "            X2, Y1, Y2, IouS = roi_helpers.calc_iou(R, img_data, C, class_mapping)\n",
    "\n",
    "            if X2 is None:\n",
    "                rpn_accuracy_rpn_monitor.append(0)\n",
    "                rpn_accuracy_for_epoch.append(0)\n",
    "                continue\n",
    "\n",
    "            neg_samples = np.where(Y1[0, :, -1] == 1)\n",
    "            pos_samples = np.where(Y1[0, :, -1] == 0)\n",
    "\n",
    "            if len(neg_samples) > 0:\n",
    "                neg_samples = neg_samples[0]\n",
    "            else:\n",
    "                neg_samples = []\n",
    "\n",
    "            if len(pos_samples) > 0:\n",
    "                pos_samples = pos_samples[0]\n",
    "            else:\n",
    "                pos_samples = []\n",
    "            \n",
    "            rpn_accuracy_rpn_monitor.append(len(pos_samples))\n",
    "            rpn_accuracy_for_epoch.append((len(pos_samples)))\n",
    "            \n",
    "            # 분류기 훈련\n",
    "            if C.num_rois > 1:\n",
    "                if len(pos_samples) < C.num_rois//2:\n",
    "                    selected_pos_samples = pos_samples.tolist()\n",
    "                else:\n",
    "                    selected_pos_samples = np.random.choice(pos_samples, C.num_rois//2, replace=False).tolist()\n",
    "                try:\n",
    "                    selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=False).tolist()\n",
    "                except:\n",
    "                    selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=True).tolist()\n",
    "\n",
    "                sel_samples = selected_pos_samples + selected_neg_samples\n",
    "            else:\n",
    "                selected_pos_samples = pos_samples.tolist()\n",
    "                selected_neg_samples = neg_samples.tolist()\n",
    "                if np.random.randint(0, 2):\n",
    "                    sel_samples = random.choice(neg_samples)\n",
    "                else:\n",
    "                    sel_samples = random.choice(pos_samples)\n",
    "\n",
    "            loss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n",
    "\n",
    "            losses[iter_num, 0] = loss_rpn[1]\n",
    "            losses[iter_num, 1] = loss_rpn[2]\n",
    "\n",
    "            losses[iter_num, 2] = loss_class[1]\n",
    "            losses[iter_num, 3] = loss_class[2]\n",
    "            losses[iter_num, 4] = loss_class[3]\n",
    "\n",
    "            iter_num += 1\n",
    "\n",
    "            progbar.update(iter_num, [('rpn_cls', np.mean(losses[:iter_num, 0])), ('rpn_regr', np.mean(losses[:iter_num, 1])),\n",
    "                                      ('detector_cls', np.mean(losses[:iter_num, 2])), ('detector_regr', np.mean(losses[:iter_num, 3]))])\n",
    "\n",
    "            if iter_num == epoch_len:\n",
    "                loss_rpn_cls = np.mean(losses[:, 0])\n",
    "                loss_rpn_regr = np.mean(losses[:, 1])\n",
    "                loss_class_cls = np.mean(losses[:, 2])\n",
    "                loss_class_regr = np.mean(losses[:, 3])\n",
    "                class_acc = np.mean(losses[:, 4])\n",
    "\n",
    "                mean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n",
    "                rpn_accuracy_for_epoch = []\n",
    "\n",
    "                if C.verbose:\n",
    "                    print('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n",
    "                    print('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n",
    "                    print('Loss RPN classifier: {}'.format(loss_rpn_cls))\n",
    "                    print('Loss RPN regression: {}'.format(loss_rpn_regr))\n",
    "                    print('Loss Detector classifier: {}'.format(loss_class_cls))\n",
    "                    print('Loss Detector regression: {}'.format(loss_class_regr))\n",
    "                    print('Elapsed time: {}'.format(time.time() - start_time))\n",
    "\n",
    "                curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n",
    "                iter_num = 0\n",
    "                start_time = time.time()\n",
    "\n",
    "                if curr_loss < best_loss:\n",
    "                    if C.verbose:\n",
    "                        print('Total loss decreased from {} to {}, saving weights'.format(best_loss,curr_loss))\n",
    "                    best_loss = curr_loss\n",
    "                    model_all.save_weights(C.model_path)\n",
    "\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Exception: {}'.format(e))\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
